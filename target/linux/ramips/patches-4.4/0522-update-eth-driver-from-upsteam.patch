--- a/drivers/net/ethernet/mediatek/esw_rt3050.c
+++ b/drivers/net/ethernet/mediatek/esw_rt3050.c
@@ -1358,8 +1358,8 @@ static int esw_probe(struct platform_device *pdev)
 	esw->dev = &pdev->dev;
 	esw->irq = irq_of_parse_and_map(np, 0);
 	esw->base = devm_ioremap_resource(&pdev->dev, res);
-	if (!esw->base)
-		return -EADDRNOTAVAIL;
+	if (IS_ERR(esw->base))
+		return PTR_ERR(esw->base);
 
 	port_map = of_get_property(np, "mediatek,portmap", NULL);
 	if (port_map)
--- a/drivers/net/ethernet/mediatek/gsw_mt7620.c
+++ b/drivers/net/ethernet/mediatek/gsw_mt7620.c
@@ -166,6 +166,7 @@ static void mt7620_hw_init(struct mt7620_gsw *gsw, struct device_node *np)
 		_mt7620_mii_write(gsw, 4, 30, 0xa000);
 		_mt7620_mii_write(gsw, 4, 4, 0x05e1);
 		_mt7620_mii_write(gsw, 4, 16, 0x1313);
+		_mt7620_mii_write(gsw, 4, 0, 0x3100);
 		pr_info("gsw: setting port4 to ephy mode\n");
 	}
 }
@@ -214,8 +215,8 @@ static int mt7620_gsw_probe(struct platform_device *pdev)
 		return -ENOMEM;
 
 	gsw->base = devm_ioremap_resource(&pdev->dev, res);
-	if (!gsw->base)
-		return -EADDRNOTAVAIL;
+	if (IS_ERR(gsw->base))
+		return PTR_ERR(gsw->base);
 
 	gsw->dev = &pdev->dev;
 
--- a/drivers/net/ethernet/mediatek/gsw_mt7621.c
+++ b/drivers/net/ethernet/mediatek/gsw_mt7621.c
@@ -41,6 +41,7 @@ static irqreturn_t gsw_interrupt_mt7621(int irq, void *_priv)
 	u32 reg, i;
 
 	reg = mt7530_mdio_r32(gsw, 0x700c);
+	mt7530_mdio_w32(gsw, 0x700c, reg);
 
 	for (i = 0; i < 5; i++)
 		if (reg & BIT(i)) {
@@ -61,7 +62,6 @@ static irqreturn_t gsw_interrupt_mt7621(int irq, void *_priv)
 		}
 
 	mt7620_handle_carrier(priv);
-	mt7530_mdio_w32(gsw, 0x700c, 0x1f);
 
 	return IRQ_HANDLED;
 }
@@ -98,15 +98,9 @@ static void mt7621_hw_init(struct mt7620_gsw *gsw, struct device_node *np)
 	mt7530_mdio_w32(gsw, 0x7000, 0x3);
 	usleep_range(10, 20);
 
-	if ((rt_sysc_r32(SYSC_REG_CHIP_REV_ID) & 0xFFFF) == 0x0101) {
-		/* (GE1, Force 1000M/FD, FC ON, MAX_RX_LENGTH 2k) */
-		mtk_switch_w32(gsw, 0x2305e30b, GSW_REG_MAC_P0_MCR);
-		mt7530_mdio_w32(gsw, 0x3600, 0x5e30b);
-	} else {
-		/* (GE1, Force 1000M/FD, FC ON, MAX_RX_LENGTH 2k) */
-		mtk_switch_w32(gsw, 0x2305e33b, GSW_REG_MAC_P0_MCR);
-		mt7530_mdio_w32(gsw, 0x3600, 0x5e33b);
-	}
+	/* (GE1, Force 1000M/FD, FC OFF, MAX_RX_LENGTH 1536) */
+	mtk_switch_w32(gsw, 0x2305e30b, GSW_REG_MAC_P0_MCR);
+	mt7530_mdio_w32(gsw, 0x3600, 0x5e30b);
 
 	/* (GE2, Link down) */
 	mtk_switch_w32(gsw, 0x8000, GSW_REG_MAC_P1_MCR);
@@ -186,6 +180,22 @@ static void mt7621_hw_init(struct mt7620_gsw *gsw, struct device_node *np)
 	mt7530_mdio_w32(gsw, 0x7a74, 0x44);
 	mt7530_mdio_w32(gsw, 0x7a7c, 0x44);
 
+	/* Disable EEE */
+	for (i = 0; i <= 4; i++) {
+		_mt7620_mii_write(gsw, i, 13, 0x7);
+		_mt7620_mii_write(gsw, i, 14, 0x3C);
+		_mt7620_mii_write(gsw, i, 13, 0x4007);
+		_mt7620_mii_write(gsw, i, 14, 0x0);
+	}
+
+	/* Disable EEE 10Base-Te */
+	for (i = 0; i <= 4; i++) {
+		_mt7620_mii_write(gsw, i, 13, 0x1f);
+		_mt7620_mii_write(gsw, i, 14, 0x027b);
+		_mt7620_mii_write(gsw, i, 13, 0x401f);
+		_mt7620_mii_write(gsw, i, 14, 0x1177);
+	}
+
 	/* turn on all PHYs */
 	for (i = 0; i <= 4; i++) {
 		val = _mt7620_mii_read(gsw, i, 0);
@@ -194,6 +204,7 @@ static void mt7621_hw_init(struct mt7620_gsw *gsw, struct device_node *np)
 	}
 
 	/* enable irq */
+	mt7530_mdio_w32(gsw, 0x7008, 0x1f);
 	val = mt7530_mdio_r32(gsw, 0x7808);
 	val |= 3 << 16;
 	mt7530_mdio_w32(gsw, 0x7808, val);
@@ -220,42 +231,34 @@ int mtk_gsw_init(struct fe_priv *priv)
 	gsw = platform_get_drvdata(pdev);
 	priv->soc->swpriv = gsw;
 
-	mt7621_hw_init(gsw, np);
-
 	if (gsw->irq) {
 		request_irq(gsw->irq, gsw_interrupt_mt7621, 0,
 			    "gsw", priv);
-		mt7530_mdio_w32(gsw, 0x7008, 0x1f);
+		disable_irq(gsw->irq);
 	}
 
+	mt7621_hw_init(gsw, np);
+
+	if (gsw->irq)
+		enable_irq(gsw->irq);
+
 	return 0;
 }
 
 static int mt7621_gsw_probe(struct platform_device *pdev)
 {
 	struct resource *res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
-	const char *port4 = NULL;
 	struct mt7620_gsw *gsw;
-	struct device_node *np;
 
 	gsw = devm_kzalloc(&pdev->dev, sizeof(struct mt7620_gsw), GFP_KERNEL);
 	if (!gsw)
 		return -ENOMEM;
 
 	gsw->base = devm_ioremap_resource(&pdev->dev, res);
-	if (!gsw->base)
-		return -EADDRNOTAVAIL;
+	if (IS_ERR(gsw->base))
+		return PTR_ERR(gsw->base);
 
 	gsw->dev = &pdev->dev;
-
-	of_property_read_string(np, "mediatek,port4", &port4);
-	if (port4 && !strcmp(port4, "ephy"))
-		gsw->port4 = PORT4_EPHY;
-	else if (port4 && !strcmp(port4, "gmac"))
-		gsw->port4 = PORT4_EXT;
-	else
-		gsw->port4 = PORT4_EPHY;
-
 	gsw->irq = platform_get_irq(pdev, 0);
 
 	platform_set_drvdata(pdev, gsw);
--- a/drivers/net/ethernet/mediatek/mdio.c
+++ b/drivers/net/ethernet/mediatek/mdio.c
@@ -57,6 +57,7 @@ static void fe_phy_link_adjust(struct net_device *dev)
 			}
 		}
 	}
+	spin_unlock_irqrestore(&priv->phy->lock, flags);
 }
 
 int fe_connect_phy_node(struct fe_priv *priv, struct device_node *phy_node)
@@ -84,7 +85,7 @@ int fe_connect_phy_node(struct fe_priv *priv, struct device_node *phy_node)
 	if (IS_ERR(phydev)) {
 		dev_err(priv->device, "could not connect to PHY\n");
 		priv->phy->phy_node[port] = NULL;
-		return PTR_ERR(phydev);
+		return -ENODEV;
 	}
 
 	phydev->supported &= PHY_GBIT_FEATURES;
@@ -109,7 +110,8 @@ static void phy_init(struct fe_priv *priv, struct phy_device *phy)
 	phy->autoneg = AUTONEG_ENABLE;
 	phy->speed = 0;
 	phy->duplex = 0;
-	phy->supported &= PHY_BASIC_FEATURES;
+	phy->supported &= IS_ENABLED(CONFIG_NET_MEDIATEK_MDIO_MT7620) ?
+			PHY_GBIT_FEATURES : PHY_BASIC_FEATURES;
 	phy->advertising = phy->supported | ADVERTISED_Autoneg;
 
 	phy_start_aneg(phy);
--- a/drivers/net/ethernet/mediatek/mtk_eth_soc.c
+++ b/drivers/net/ethernet/mediatek/mtk_eth_soc.c
@@ -205,8 +205,9 @@ static inline void fe_set_txd(struct fe_tx_dma *txd, struct fe_tx_dma *dma_txd)
 
 static void fe_clean_rx(struct fe_priv *priv)
 {
-	int i;
 	struct fe_rx_ring *ring = &priv->rx_ring;
+	struct page *page;
+	int i;
 
 	if (ring->rx_data) {
 		for (i = 0; i < ring->rx_ring_size; i++)
@@ -216,7 +217,7 @@ static void fe_clean_rx(struct fe_priv *priv)
 							 ring->rx_dma[i].rxd1,
 							 ring->rx_buf_size,
 							 DMA_FROM_DEVICE);
-				put_page(virt_to_head_page(ring->rx_data[i]));
+				skb_free_frag(ring->rx_data[i]);
 			}
 
 		kfree(ring->rx_data);
@@ -230,6 +231,13 @@ static void fe_clean_rx(struct fe_priv *priv)
 				  ring->rx_phys);
 		ring->rx_dma = NULL;
 	}
+
+	if (!ring->frag_cache.va)
+	    return;
+
+	page = virt_to_page(ring->frag_cache.va);
+	__page_frag_cache_drain(page, ring->frag_cache.pagecnt_bias);
+	memset(&ring->frag_cache, 0, sizeof(ring->frag_cache));
 }
 
 static int fe_alloc_rx(struct fe_priv *priv)
@@ -244,7 +252,9 @@ static int fe_alloc_rx(struct fe_priv *priv)
 		goto no_rx_mem;
 
 	for (i = 0; i < ring->rx_ring_size; i++) {
-		ring->rx_data[i] = netdev_alloc_frag(ring->frag_size);
+		ring->rx_data[i] = page_frag_alloc(&ring->frag_cache,
+						   ring->frag_size,
+						   GFP_KERNEL);
 		if (!ring->rx_data[i])
 			goto no_rx_mem;
 	}
@@ -463,9 +473,9 @@ static struct rtnl_link_stats64 *fe_get_stats64(struct net_device *dev,
 	}
 
 	if (netif_running(dev) && netif_device_present(dev)) {
-		if (spin_trylock(&hwstats->stats_lock)) {
+		if (spin_trylock_bh(&hwstats->stats_lock)) {
 			fe_stats_update(priv);
-			spin_unlock(&hwstats->stats_lock);
+			spin_unlock_bh(&hwstats->stats_lock);
 		}
 	}
 
@@ -544,7 +554,7 @@ static inline u32 fe_empty_txd(struct fe_tx_ring *ring)
 	barrier();
 	return (u32)(ring->tx_ring_size -
 			((ring->tx_next_idx - ring->tx_free_idx) &
-			 (ring->tx_ring_size - 1)));
+			 (ring->tx_ring_size - 1)) - 1);
 }
 
 static int fe_tx_map_dma(struct sk_buff *skb, struct net_device *dev,
@@ -774,6 +784,11 @@ static int fe_start_xmit(struct sk_buff *skb, struct net_device *dev)
 	int tx_num;
 	int len = skb->len;
 
+	if (unlikely(test_bit(__FE_DOWN, &priv->state))) {
+		dev_kfree_skb_any(skb);
+		return NETDEV_TX_OK;
+	}
+
 	if (fe_skb_padto(skb, priv)) {
 		netif_warn(priv, tx_err, dev, "tx padding failed!\n");
 		return NETDEV_TX_OK;
@@ -834,7 +849,8 @@ static int fe_poll_rx(struct napi_struct *napi, int budget,
 			break;
 
 		/* alloc new buffer */
-		new_data = netdev_alloc_frag(ring->frag_size);
+		new_data = page_frag_alloc(&ring->frag_cache, ring->frag_size,
+					   GFP_ATOMIC);
 		if (unlikely(!new_data)) {
 			stats->rx_dropped++;
 			goto release_desc;
@@ -844,14 +860,14 @@ static int fe_poll_rx(struct napi_struct *napi, int budget,
 					  ring->rx_buf_size,
 					  DMA_FROM_DEVICE);
 		if (unlikely(dma_mapping_error(&netdev->dev, dma_addr))) {
-			put_page(virt_to_head_page(new_data));
+			skb_free_frag(new_data);
 			goto release_desc;
 		}
 
 		/* receive data */
 		skb = build_skb(data, ring->frag_size);
 		if (unlikely(!skb)) {
-			put_page(virt_to_head_page(new_data));
+			skb_free_frag(new_data);
 			goto release_desc;
 		}
 		skb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN);
@@ -882,17 +898,17 @@ release_desc:
 			rxd->rxd2 = RX_DMA_LSO;
 
 		ring->rx_calc_idx = idx;
+		done++;
+	}
+
+	if (done) {
 		/* make sure that all changes to the dma ring are flushed before
 		 * we continue
 		 */
 		wmb();
 		fe_reg_w32(ring->rx_calc_idx, FE_REG_RX_CALC_IDX0);
-		done++;
 	}
 
-	if (done < budget)
-		fe_reg_w32(rx_intr, FE_REG_FE_INT_STATUS);
-
 	return done;
 }
 
@@ -928,14 +944,7 @@ static int fe_poll_tx(struct fe_priv *priv, int budget, u32 tx_intr,
 	}
 	ring->tx_free_idx = idx;
 
-	if (idx == hwidx) {
-		/* read hw index again make sure no new tx packet */
-		hwidx = fe_reg_r32(FE_REG_TX_DTX_IDX0);
-		if (idx == hwidx)
-			fe_reg_w32(tx_intr, FE_REG_FE_INT_STATUS);
-		else
-			*tx_again = 1;
-	} else {
+	if (idx != hwidx && done) {
 		*tx_again = 1;
 	}
 
@@ -958,6 +967,11 @@ static int fe_poll(struct napi_struct *napi, int budget)
 	u32 status, fe_status, status_reg, mask;
 	u32 tx_intr, rx_intr, status_intr;
 
+	if (unlikely(test_bit(__FE_DOWN, &priv->state))) {
+		napi_complete(napi);
+		return 0;
+	}
+
 	status = fe_reg_r32(FE_REG_FE_INT_STATUS);
 	fe_status = status;
 	tx_intr = priv->soc->tx_int;
@@ -967,6 +981,10 @@ static int fe_poll(struct napi_struct *napi, int budget)
 	rx_done = 0;
 	tx_again = 0;
 
+	if ((status & (tx_intr | rx_intr))) {
+		fe_reg_w32((status & (tx_intr | rx_intr)), FE_REG_FE_INT_STATUS);
+	}
+
 	if (fe_reg_table[FE_REG_FE_INT_STATUS2]) {
 		fe_status = fe_reg_r32(FE_REG_FE_INT_STATUS2);
 		status_reg = FE_REG_FE_INT_STATUS2;
@@ -974,11 +992,15 @@ static int fe_poll(struct napi_struct *napi, int budget)
 		status_reg = FE_REG_FE_INT_STATUS;
 	}
 
-	if (status & tx_intr)
-		tx_done = fe_poll_tx(priv, budget, tx_intr, &tx_again);
+	tx_done = fe_poll_tx(priv, budget, tx_intr, &tx_again);
+	if (tx_done && !tx_again && !(status & tx_intr)) {
+		fe_reg_w32(tx_intr, FE_REG_FE_INT_STATUS);
+	}
 
-	if (status & rx_intr)
-		rx_done = fe_poll_rx(napi, budget, priv, rx_intr);
+	rx_done = fe_poll_rx(napi, budget, priv, rx_intr);
+	if (rx_done && rx_done < budget && !(status & rx_intr)) {
+		fe_reg_w32(rx_intr, FE_REG_FE_INT_STATUS);
+	}
 
 	if (unlikely(fe_status & status_intr)) {
 		if (hwstat && spin_trylock(&hwstat->stats_lock)) {
@@ -996,20 +1018,22 @@ static int fe_poll(struct napi_struct *napi, int budget)
 	}
 
 	if (!tx_again && (rx_done < budget)) {
-		status = fe_reg_r32(FE_REG_FE_INT_STATUS);
-		if (status & (tx_intr | rx_intr)) {
-			/* let napi poll again */
-			rx_done = budget;
-			goto poll_again;
-		}
+		bool napi_complete_done_status;
 
-		napi_complete(napi);
-		fe_int_enable(tx_intr | rx_intr);
+		napi_complete_done_status=!unlikely(test_bit(NAPI_STATE_NPSVC, &napi->state));
+
+		if (napi_complete_done_status)
+		{
+			napi_complete_done(napi, rx_done);
+		}
+		
+		if (likely(napi_complete_done_status && !test_bit(__FE_DOWN, &priv->state))) {
+			fe_int_enable(tx_intr | rx_intr);
+		}
 	} else {
 		rx_done = budget;
 	}
 
-poll_again:
 	return rx_done;
 }
 
@@ -1157,7 +1181,8 @@ static int fe_hw_init(struct net_device *dev)
 	/* disable delay interrupt */
 	fe_reg_w32(0, FE_REG_DLY_INT_CFG);
 
-	fe_int_disable(priv->soc->tx_int | priv->soc->rx_int);
+	/* disable all interrupts during hw init */
+	fe_int_disable(~0);
 
 	/* frame engine will push VLAN tag regarding to VIDX feild in Tx desc */
 	if (fe_reg_table[FE_REG_FE_DMA_VID_BASE])
@@ -1180,25 +1205,6 @@ static int fe_hw_init(struct net_device *dev)
 static int fe_open(struct net_device *dev)
 {
 	struct fe_priv *priv = netdev_priv(dev);
-	unsigned long flags;
-	u32 val;
-	int err;
-
-	err = fe_init_dma(priv);
-	if (err) {
-		fe_free_dma(priv);
-		return err;
-	}
-
-	spin_lock_irqsave(&priv->page_lock, flags);
-
-	val = FE_TX_WB_DDONE | FE_RX_DMA_EN | FE_TX_DMA_EN;
-	if (priv->flags & FE_FLAG_RX_2B_OFFSET)
-		val |= FE_RX_2B_OFFSET;
-	val |= priv->soc->pdma_glo_cfg;
-	fe_reg_w32(val, FE_REG_PDMA_GLO_CFG);
-
-	spin_unlock_irqrestore(&priv->page_lock, flags);
 
 	if (priv->phy)
 		priv->phy->start(priv);
@@ -1206,6 +1212,8 @@ static int fe_open(struct net_device *dev)
 	if (priv->soc->has_carrier && priv->soc->has_carrier(priv))
 		netif_carrier_on(dev);
 
+	clear_bit(__FE_DOWN, &priv->state);
+
 	napi_enable(&priv->rx_napi);
 	fe_int_enable(priv->soc->tx_int | priv->soc->rx_int);
 	netif_start_queue(dev);
@@ -1216,9 +1224,10 @@ static int fe_open(struct net_device *dev)
 static int fe_stop(struct net_device *dev)
 {
 	struct fe_priv *priv = netdev_priv(dev);
-	unsigned long flags;
 	int i;
 
+	set_bit(__FE_DOWN, &priv->state);
+
 	netif_tx_disable(dev);
 	fe_int_disable(priv->soc->tx_int | priv->soc->rx_int);
 	napi_disable(&priv->rx_napi);
@@ -1226,13 +1235,6 @@ static int fe_stop(struct net_device *dev)
 	if (priv->phy)
 		priv->phy->stop(priv);
 
-	spin_lock_irqsave(&priv->page_lock, flags);
-
-	fe_reg_w32(fe_reg_r32(FE_REG_PDMA_GLO_CFG) &
-		     ~(FE_TX_WB_DDONE | FE_RX_DMA_EN | FE_TX_DMA_EN),
-		     FE_REG_PDMA_GLO_CFG);
-	spin_unlock_irqrestore(&priv->page_lock, flags);
-
 	/* wait dma stop */
 	for (i = 0; i < 10; i++) {
 		if (fe_reg_r32(FE_REG_PDMA_GLO_CFG) &
@@ -1243,8 +1245,6 @@ static int fe_stop(struct net_device *dev)
 		break;
 	}
 
-	fe_free_dma(priv);
-
 	return 0;
 }
 
@@ -1253,6 +1253,8 @@ static int __init fe_init(struct net_device *dev)
 	struct fe_priv *priv = netdev_priv(dev);
 	struct device_node *port;
 	const char *mac_addr;
+	unsigned long flags;
+	u32 val;
 	int err;
 
 	priv->soc->reset_fe();
@@ -1297,6 +1299,21 @@ static int __init fe_init(struct net_device *dev)
 	if ((priv->flags & FE_FLAG_HAS_SWITCH) && priv->soc->switch_config)
 		priv->soc->switch_config(priv);
 
+	set_bit(__FE_DOWN, &priv->state);
+	err = fe_init_dma(priv);
+	if (err) {
+		fe_free_dma(priv);
+		goto err_phy_disconnect;
+	}
+
+	spin_lock_irqsave(&priv->page_lock, flags);
+	val = FE_TX_WB_DDONE | FE_RX_DMA_EN | FE_TX_DMA_EN;
+	if (priv->flags & FE_FLAG_RX_2B_OFFSET)
+		val |= FE_RX_2B_OFFSET;
+	val |= priv->soc->pdma_glo_cfg;
+	fe_reg_w32(val, FE_REG_PDMA_GLO_CFG);
+	spin_unlock_irqrestore(&priv->page_lock, flags);
+
 	return 0;
 
 err_phy_disconnect:
@@ -1309,8 +1326,16 @@ err_phy_disconnect:
 
 static void fe_uninit(struct net_device *dev)
 {
+	unsigned long flags;
 	struct fe_priv *priv = netdev_priv(dev);
 
+	spin_lock_irqsave(&priv->page_lock, flags);
+	fe_reg_w32(fe_reg_r32(FE_REG_PDMA_GLO_CFG) &
+		     ~(FE_TX_WB_DDONE | FE_RX_DMA_EN | FE_TX_DMA_EN),
+		     FE_REG_PDMA_GLO_CFG);
+	spin_unlock_irqrestore(&priv->page_lock, flags);
+	fe_free_dma(priv);
+
 	if (priv->phy)
 		priv->phy->disconnect(priv);
 	fe_mdio_cleanup(priv);
@@ -1467,7 +1492,7 @@ static int fe_probe(struct platform_device *pdev)
 		soc->reg_table = fe_reg_table;
 
 	fe_base = devm_ioremap_resource(&pdev->dev, res);
-	if (!fe_base) {
+	if (IS_ERR(fe_base)) {
 		err = -EADDRNOTAVAIL;
 		goto err_out;
 	}
@@ -1535,6 +1560,7 @@ static int fe_probe(struct platform_device *pdev)
 	priv->tx_ring.tx_ring_size = NUM_DMA_DESC;
 	priv->rx_ring.rx_ring_size = NUM_DMA_DESC;
 	INIT_WORK(&priv->pending_work, fe_pending_work);
+	u64_stats_init(&priv->hw_stats->syncp);
 
 	napi_weight = 32;
 	if (priv->flags & FE_FLAG_NAPI_WEIGHT) {
--- a/drivers/net/ethernet/mediatek/mtk_eth_soc.h
+++ b/drivers/net/ethernet/mediatek/mtk_eth_soc.h
@@ -461,6 +461,7 @@ struct fe_tx_ring {
 };
 
 struct fe_rx_ring {
+	struct page_frag_cache frag_cache;
 	struct fe_rx_dma *rx_dma;
 	u8 **rx_data;
 	dma_addr_t rx_phys;
@@ -470,10 +471,16 @@ struct fe_rx_ring {
 	u16 rx_calc_idx;
 };
 
+enum fe_state_t {
+	__FE_DOWN,
+};
+
 struct fe_priv {
 	/* make sure that register operations are atomic */
 	spinlock_t			page_lock;
 
+	unsigned long state;
+
 	struct fe_soc_data		*soc;
 	struct net_device		*netdev;
 	struct device_node		*switch_np;
